{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vinafood_dataset import get_data_loaders\n",
    "from ResNet import ResNet\n",
    "from GoogLeNet import GoogLeNet\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "\n",
    "# Model\n",
    "def _make_model(model: str, num_classes: int, lr: float):   # GoogLeNet, ResNet\n",
    "    if model == \"GoogLeNet\":\n",
    "        model = GoogLeNet(n_classes=num_classes).to(device)\n",
    "    elif model == \"ResNet\":\n",
    "        model = ResNet(num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Model name không hợp lệ. Chỉ chấp nhận 'GoogLeNet' hoặc 'ResNet'.\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LR\n",
    "    )\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, loss_fn, optimizer, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Optimize zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward Pass\n",
    "        output_pred = model(images)\n",
    "        \n",
    "        # 3. Calculate the loss\n",
    "        loss = loss_fn(output_pred, labels)\n",
    "        \n",
    "        # 4. Perform backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Step the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 6. Update total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluate Function\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output_pred = model(images)\n",
    "            _, preds = torch.max(output_pred, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def training_loop(\n",
    "    epochs, model, model_name, loss_fn,\n",
    "    optimizer, train_loader, test_loader\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(\n",
    "            model=model, \n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            loader=train_loader\n",
    "        )\n",
    "        acc, prec, rec, f1 = evaluate_model(model=model, loader=test_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1-score: {f1:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    torch.save (model.state_dict(), f\"{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "Train Loss: 2.6086\n",
      "Accuracy: 0.1883 | Precision: 0.1808 | Recall: 0.1918 | F1-score: 0.1433\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30]\n",
      "Train Loss: 2.3267\n",
      "Accuracy: 0.2595 | Precision: 0.2468 | Recall: 0.2091 | F1-score: 0.1822\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30]\n",
      "Train Loss: 2.1198\n",
      "Accuracy: 0.2921 | Precision: 0.2975 | Recall: 0.2604 | F1-score: 0.2391\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30]\n",
      "Train Loss: 1.9872\n",
      "Accuracy: 0.3376 | Precision: 0.3519 | Recall: 0.3225 | F1-score: 0.2919\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30]\n",
      "Train Loss: 1.8362\n",
      "Accuracy: 0.3319 | Precision: 0.3894 | Recall: 0.3068 | F1-score: 0.2855\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30]\n",
      "Train Loss: 1.7437\n",
      "Accuracy: 0.3985 | Precision: 0.4092 | Recall: 0.3649 | F1-score: 0.3455\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30]\n",
      "Train Loss: 1.6223\n",
      "Accuracy: 0.4228 | Precision: 0.4077 | Recall: 0.3927 | F1-score: 0.3732\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30]\n",
      "Train Loss: 1.5258\n",
      "Accuracy: 0.4466 | Precision: 0.4682 | Recall: 0.4123 | F1-score: 0.4031\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30]\n",
      "Train Loss: 1.4024\n",
      "Accuracy: 0.4704 | Precision: 0.4918 | Recall: 0.4414 | F1-score: 0.4401\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30]\n",
      "Train Loss: 1.3049\n",
      "Accuracy: 0.4600 | Precision: 0.4995 | Recall: 0.4251 | F1-score: 0.4327\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30]\n",
      "Train Loss: 1.2311\n",
      "Accuracy: 0.4734 | Precision: 0.5221 | Recall: 0.4612 | F1-score: 0.4582\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30]\n",
      "Train Loss: 1.1255\n",
      "Accuracy: 0.5057 | Precision: 0.4970 | Recall: 0.4952 | F1-score: 0.4852\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30]\n",
      "Train Loss: 1.0371\n",
      "Accuracy: 0.5096 | Precision: 0.5586 | Recall: 0.4881 | F1-score: 0.4852\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30]\n",
      "Train Loss: 0.9591\n",
      "Accuracy: 0.5305 | Precision: 0.5475 | Recall: 0.5183 | F1-score: 0.5118\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30]\n",
      "Train Loss: 0.8441\n",
      "Accuracy: 0.5094 | Precision: 0.5578 | Recall: 0.5010 | F1-score: 0.4904\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30]\n",
      "Train Loss: 0.7712\n",
      "Accuracy: 0.4990 | Precision: 0.5358 | Recall: 0.4987 | F1-score: 0.4706\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30]\n",
      "Train Loss: 0.6834\n",
      "Accuracy: 0.5019 | Precision: 0.5594 | Recall: 0.5144 | F1-score: 0.5034\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30]\n",
      "Train Loss: 0.5938\n",
      "Accuracy: 0.5621 | Precision: 0.5796 | Recall: 0.5610 | F1-score: 0.5556\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30]\n",
      "Train Loss: 0.5451\n",
      "Accuracy: 0.5409 | Precision: 0.5724 | Recall: 0.5244 | F1-score: 0.5287\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30]\n",
      "Train Loss: 0.4820\n",
      "Accuracy: 0.5198 | Precision: 0.5466 | Recall: 0.5306 | F1-score: 0.5144\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30]\n",
      "Train Loss: 0.4395\n",
      "Accuracy: 0.5790 | Precision: 0.5773 | Recall: 0.5734 | F1-score: 0.5681\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30]\n",
      "Train Loss: 0.3713\n",
      "Accuracy: 0.5658 | Precision: 0.5776 | Recall: 0.5483 | F1-score: 0.5481\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30]\n",
      "Train Loss: 0.3442\n",
      "Accuracy: 0.5564 | Precision: 0.5925 | Recall: 0.5434 | F1-score: 0.5446\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30]\n",
      "Train Loss: 0.3003\n",
      "Accuracy: 0.5691 | Precision: 0.5869 | Recall: 0.5647 | F1-score: 0.5496\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30]\n",
      "Train Loss: 0.2635\n",
      "Accuracy: 0.5796 | Precision: 0.5944 | Recall: 0.5700 | F1-score: 0.5659\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30]\n",
      "Train Loss: 0.2407\n",
      "Accuracy: 0.5956 | Precision: 0.5909 | Recall: 0.5936 | F1-score: 0.5853\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30]\n",
      "Train Loss: 0.2388\n",
      "Accuracy: 0.6024 | Precision: 0.6115 | Recall: 0.5915 | F1-score: 0.5937\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30]\n",
      "Train Loss: 0.2045\n",
      "Accuracy: 0.5245 | Precision: 0.5533 | Recall: 0.5289 | F1-score: 0.5162\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30]\n",
      "Train Loss: 0.2161\n",
      "Accuracy: 0.5663 | Precision: 0.5866 | Recall: 0.5567 | F1-score: 0.5550\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kittnguyen/Downloads/Learning_Documents/Deep_Learning/myvenv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30]\n",
      "Train Loss: 0.1978\n",
      "Accuracy: 0.5576 | Precision: 0.5942 | Recall: 0.5482 | F1-score: 0.5515\n",
      "--------------------------------------------------\n",
      "Training done, model saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Data Loader\n",
    "    train_loader, test_loader, num_classes = get_data_loaders(batch_size=BATCH_SIZE)\n",
    "\n",
    "    model, loss_fn, optimizer = _make_model(\"GoogLeNet\", num_classes, LR)\n",
    "\n",
    "    training_loop(\n",
    "        epochs=EPOCHS,\n",
    "        model=model,\n",
    "        model_name=\"vinamodel\",\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9928038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vinafood_dataset import get_data_loaders\n",
    "from ResNet import ResNet\n",
    "from GoogLeNet import GoogLeNet\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "def _make_model(model: str, num_classes: int, lr: float):   # GoogLeNet, ResNet\n",
    "    if model == \"GoogLeNet\":\n",
    "        model = GoogLeNet(n_classes=num_classes).to(device)\n",
    "    elif model == \"ResNet\":\n",
    "        model = ResNet(num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Model name không hợp lệ. Chỉ chấp nhận 'GoogLeNet' hoặc 'ResNet'.\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LR\n",
    "    )\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, loss_fn, optimizer, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Optimize zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward Pass\n",
    "        output_pred = model(images)\n",
    "        \n",
    "        # 3. Calculate the loss\n",
    "        loss = loss_fn(output_pred, labels)\n",
    "        \n",
    "        # 4. Perform backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Step the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 6. Update total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluate Function\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output_pred = model(images)\n",
    "            _, preds = torch.max(output_pred, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def training_loop(\n",
    "    epochs, model, model_name, loss_fn,\n",
    "    optimizer, train_loader, test_loader\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(\n",
    "            model=model, \n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            loader=train_loader\n",
    "        )\n",
    "        acc, prec, rec, f1 = evaluate_model(model=model, loader=test_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1-score: {f1:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    torch.save (model.state_dict(), f\"{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: 21 classes, 10044 train images, 6682 test images\n",
      "Epoch [1/10]\n",
      "Train Loss: 2.6195\n",
      "Accuracy: 0.2287 | Precision: 0.2670 | Recall: 0.2197 | F1-score: 0.2000\n",
      "--------------------------------------------------\n",
      "Epoch [2/10]\n",
      "Train Loss: 2.3494\n",
      "Accuracy: 0.2415 | Precision: 0.2724 | Recall: 0.2362 | F1-score: 0.2003\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Data Loader\n",
    "    train_loader, test_loader, num_classes = get_data_loaders(batch_size=BATCH_SIZE)\n",
    "\n",
    "    model, loss_fn, optimizer = _make_model(\"ResNet\", num_classes, LR)\n",
    "\n",
    "    training_loop(\n",
    "        epochs=EPOCHS,\n",
    "        model=model,\n",
    "        model_name=\"resnet\",\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff5f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
